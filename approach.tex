\subsection{Approach}
The new vtkGPUVolumeRayCastMapper uses a ray casting technique ~\ref{raycasting} for volume rendering which is a state-of-the-art for volume rendering on modern graphics platforms. Algorithmically, at a high level, it is similar to the older version of this class (although with a fairly different OpenGL implementation since that original class was first written over a decade ago and used GPU assembly code).  One of the main reason we chose to use ray casting due to the flexibility of this technique, which enables us to support all the features of the software ray cast mapper but with the acceleration of the GPU. Ray casting is an image-order rendering technique, with one or more rays cast through the volume per image pixel. VTK is inherently an object-order rendering system, where the GPU renders all graphical primitives (points, lines, triangles, etc.) represented by vtkProp(s) in the scene in one or more passes (with multiple passes needed to support advanced features such as depth peeling for transparency).

The image-order rendering process for vtkVolume is initiated when the front-facing polygons of the volumeâ€™s bounding box are rendered with a custom fragment program. This fragment program is used to cast a ray through the volume at each pixel, with the fragment location indicating the starting location for that ray. The volume and all the various rendering parameters are transferred to the GPU through the use of textures (3D for the volume, 1D for the various transfer functions) and uniform variables. Steps are taken along the ray until the ray exits the volume, and the resulting computed color and opacity are blended into the current pixel value. Note that volumes are rendered after all opaque geometry in the scene to allow the ray casting process to terminate at the depth value stored in the depth buffer for that pixel (and, hence, correctly intermix with opaque geometry).

In addition to providing supported features of the old mapper, the new mapper added new capabilities such as clipping on GPU,  gradient opacity, and volume picking amongst many others. In the next few sections, we will cover each of these features in detail.

\subsubsection{Single Pass}
\begin{figure}
\centering
\includegraphics[width=3in]{frontandback.png}
\caption{Front and back faces are rendered for start and end position of the ray.}
\label{fig:raycasting}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=3in]{raycasting.jpg}
\caption{Implementing volume rendering using single-pass GPU ray casting.}
\label{fig:raycasting}
\end{figure}

In a ray-casting algorithm, the entry and the exit point into the volume is needed to determine when to stop the ray-marching. To determine the entry and the exit point, one approach is to render the geometry of the volume bounding box of the volume twice. In the first pass, the front face of the geometry is rendered and in the second pass the backface is rendered as shown in ~\ref{fig:frontandback}. Using the interpolated vertex position and texture lookup, the start and end positions is computed. Instead of this, in vtkGPUVolumeRayCastMapper, entry and exit points are computed based on the fact that the texture extents of the volume is within vec3(1.0), vec3(-1.0) range (and ~\ref{fig:raycasting}). The code below is showing the fragment shader piece that determines whether or not to stop marching the rays depending on the value of stop.
 
 \begin{lstlisting}[breaklines=true]
 bool stop = any(greaterThan(g_dataPos, 
                 ip_texMax)) ||
             any(lessThan(g_dataPos, 
                 ip_texMin));
 \end{lstlisting}
 
 The advantage of such approach is that it requires one less pass and is faster than other approaches since there is no texture generation or lookup happens for determining the termination of the ray.
 
 
\subsubsection{Dynamic Shader Generation}
In the new \texttt{vtkOpenGLGPUVolumeRayCastMapper} all the operations are performed on the GPU. The advantage of this approach was a more streamlined code that is easier to maintain and debug. This approach also provided an opportunity to rework how to support different features without having too many branches in the shader code or having to send all the options to the shader because that would have been detrimental to the performance. In the new \texttt{vtkOpenGLGPUVolumeRayCastMapper}, the shader is dynamically composed by the mapper. For this to work, we have introduced tags in a vertex, or fragment shader which are then replaced by the \texttt{vtkShaderComposer} depending on the option enabled or chosen by the application code. For instance, the skeleton fragment shader defines tags as shown below:
 
 \begin{lstlisting}
//VTK::Base::Dec

//VTK::Termination::Dec 
\end{lstlisting}

At the run time then \texttt{//VTK::Base::Dec} is then replaced by the code shown below. 

To define a structure, we have chosen a strategy that separates the tags in found category: 

1. Declaration (::Dec)
The tags belong to this group are meant to declare variables or function outside the main execution of the shader code. The variables defined are uniform, varying, and user-defined global variables. The functions defined are typically perform operations that are repetitive in nature such as computing color of a fragment. 

2. Initialization (::Init)
The tags belong to this group are meant to initialize variables inside the main execution function of the shader but before the ray-casting loop in the fragment shader. An example of such code includes computation of the initial position of ray and direction of ray traversal.

3. Implementation (::Impl)
The tags belong to this group are the variables or functions or the combination of both that perform the actual operation of clipping, cropping, shading, etc. on one, two, or four component volume data. The implementation code used local and global variables and optimized for performance reasons as they are executed as long as the ray is traversing inside the volume and didn't run into a termination condition which is checked every time.  

4. Exit (::Exit)
The tags belong to this group perform final computation such as the final color of the fragment. These tags are placed outside the ray-casting loop and typically contain numeric assignments.

\subsubsection{Lighting / Shading }
The old mapper supported only one light (due to limitations in OpenGL at the time the class was written). The \texttt{vtkFixedPointRayCastMapper} supports multiple lights, but only with an approximate lighting model, since gradients are precomputed and quantized, and shading is performed for each potential gradient direction regardless of fragment location. The new mapper accurately implemented the VTK lighting model to produce high quality images for publication. To support this, depending on the light type (point, directional, and positional), the lighting parameters are sent to the shader which then performs the per pixel lighting calculations. The number of lights is limited to six mostly because of the performance reasons as the interactive performance goes down significantly with each light added to the scene. The Phong shading lighting model is used for volume rendering lighting. Phong lighting requires normals which have to be computed for each fragment. The normal calculation is done by first computing the gradient and then scaling the gradient by the spacing between the cells. The gradient is computed by reading the scalar values form the neighboring cdells using the offset vector that stores the step size based on the bounds of the volume. 

\subsubsection{Volume Picking}
Picking in the context of volume rendering is the determination of which voxel user has clicked using a pointing device such as the mouse. Picking is one of most basic operations users performs to interact with a 3D scene. VTK legacy volume mappers support picking through an instance external to the mapper itself called ~\texttt{vtkVolumePicker}.  This class casts a ray into the volume and returns the point where the ray intersects an isosurface of a user specified opacity. This technique has certain limitations given that the picking class does not have enough information to correctly account for clipping, transfer functions and other parameters which define how the mapper renders internally, this reduces its reliability on the actual objects being picked. The inherent flexibility of the glsl-based implementation of the new mapper enables seamless integration with the ~\texttt{vtkHardwareSelector}'s interface, which allows a consistent selection of objects in a scene regardless of whether it is geometric or volumetric data.  Providing picking support directly within the fragment shader of the volume mapper ensures high selection accuracy even in situations where a volume intermixes with geometry in seemingly cumbersome ways or advanced features like when clipping planes are enabled (what you see is what you pick). Given the readily available picking styles supported by ~\texttt{vtkHardwareSelector} (e.g. ~\texttt{vtkAreaPicker}), it is possible to make a selection of a specific set of voxels.

\subsubsection{Volume Texture Streaming}
An intrinsic limitation of volume rendering is that the texture to be rendered does not always fit into the graphics memory of a system. This becomes increasingly important to address now that the new mapper provides support for mobile architectures.
A relatively simple method when dealing with a large volume is the divide-and-conquer approach, which is sometimes referred to as bricking. The volume is split into several blocks in such a way that a single sub-block (brick) fits completely into GPU memory.  Each sub-block is stored in main memory and streamed into GPU memory for a rendering pass one at a time (in a back-to-front manner for correct composition). The sub-blocks are rendered using the standard shader programs and alpha-blended with each other by OpenGL.
Streaming the volume as separate texture bricks certainly imposes a performance trade-off but acts as a graphics memory expansion scheme for devices that would not be able to render a higher quality volume otherwise.

\subsubsection{Dual Depth Peeling}
 
 
\subsubsection{Render to texture}
While the vtkGPUVolumeRayCastMapper uses a single pass rendering approach, it provides a render to texture mode to support advanced graphics applications. Render to texture is a graphics technique used in various multi-pass rendering pipelines for achieving advanced visual effects. It involves using the rendered output from a pass to modify / enhance the output of other rendering passes. The volume mapper allows rendering to an OpenGL FrameBufferObject (FBO) and obtaining the pixel data from the FBO via simple image retrieval calls. Two kinds of data can be obtained from  the mapper using the render to texture mode viz. color and depth information. The color data is simply the pixel representation of the rendered volume whereas depth data consists of a grayscale image depicting how deep each voxel is in the volume. When capturing the depth information, the depth of the first non-transparent voxel is captured.

\subsubsection{Mobile Support}
The move to OpenGL 3 and higher enabled volume rendering to support mobile devices (iOS and Android devices) as OpenGL ES 3.0 and higher supports 3D textures. Support for multiple touch events such as using two fingers to translate, rotate, and zoom the camera was added to support interactive rendering on mobile devices. New texture formats are added with logistics to enable / disable them based on the platform.. With minor feature-set exceptions, the new volume mapper works on mobile devices enabling developers to build sophisticated applications for the scientific community.

\subsubsection{Optimizations and Edge-Cases}
The new \texttt{vtkOpenGLGPUVolumeRayCastMapper} is more than just a ray cast mapper implementation. It is designed to work on multiple platforms and developed to perform volume rendering at interactive frame rates. To achieve interactive frame rates and to handle edge cases, we have implemented following optimizations in the new mapper. 

\begin{itemize}
\item Clipping plane optimization 
In VTK a user can place multiple planes at desired angles to clip the volume. This technique is essential for many medical use-cases. Since only one side of clip planes needs to be traversed, performing any sort of ray cast on the clipped side is wasteful. Hence a simple optimization is to move the starting point of the ray on the plane by projecting the ray onto the plane in the view direction. 

\item Eye Too Close to the Volume
When the eye is too close to the volume, the near plane could clip the volume visible bounds resulting in OpenGL clipping removing the geometry from rendering. In such cases the bounding box of the volume that defines the texture coordinates and hence define a surface to start casting the ray needs to be clipped. To handle such scenario, a plant-box interaction is added. So far evey move of the camera is funding invokved checking if the geomtry of the volume needs to be clipped.

\item Support for Double and Long Long Data Type


\end{itemize}